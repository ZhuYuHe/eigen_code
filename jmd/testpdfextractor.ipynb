{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## pdftree\n",
    "根节点为virtualNode，根据headline及其level构建pdftree。\n",
    "没有headline的默认为根节点的子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pdfextractor import extractor\n",
    "import pdfextractor\n",
    "import pickle\n",
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#根据文章与pdf的映射，找出文章句子与pdf句子的匹配关系\n",
    "def MatchTwoText(t1, t2, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    检查两个句子是否匹配\n",
    "    \"\"\"\n",
    "    ct1 = ' '.join(jieba.cut(t1)).split()\n",
    "    ct2 = ' '.join(jieba.cut(t2)).split()\n",
    "    commonL = set(ct1).intersection(set(ct2)).__len__()\n",
    "    if commonL * 1.0 / set(ct1).__len__() > threshold:\n",
    "#         print(\"match \" + str(commonL * 1.0/ set(ct1).__len__() * 100) + '% ')\n",
    "#         print(\"文章句子:\" + '\\n' + t1)\n",
    "#         print(\"PDF句子: \" + '\\n' + t2)\n",
    "#         print('\\n')\n",
    "        return t2\n",
    "    return ''\n",
    "    \n",
    "def isMatch(text, pdfTextList, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    @parameter text: 待检查句子\n",
    "    @parameter pdfTextList: PDF句子与表格信息列表\n",
    "    @parameter threshold: 相似度为多少时返回true\n",
    "    @return： boolean值，pdf列表中是否有与待检查句子非常相似的句子\n",
    "    \"\"\"\n",
    "    for pdftext in pdfTextList:\n",
    "        s = MatchTwoText(text, pdftext, threshold)\n",
    "        if s != '':\n",
    "            return s\n",
    "    return ''\n",
    "\n",
    "#map和article中有24篇重合，24篇中有11篇有标注的\n",
    "dataJoin = pd.read_csv('/home/zhuyuhe/mydata/jmd/percent/dataJoin.txt', delimiter='\\t', header = None)\n",
    "dataJoin = dataJoin.rename(columns = {0:'文本', 1: '意图', 2: 'pdfurl'})\n",
    "pdfurls = dataJoin.pdfurl.unique()\n",
    "for url in pdfurls:\n",
    "    try :\n",
    "        pdflist = extractPDF(url, 0.8)\n",
    "    except BaseException:\n",
    "        continue\n",
    "    urldata = dataJoin[dataJoin.pdfurl == url]\n",
    "    for i in urldata.index:\n",
    "        s = isMatch(dataJoin.loc[i, '文本'], pdflist, 0.6)\n",
    "        print s\n",
    "        dataJoin.loc[i, 'pdf文本'] = s\n",
    "        \n",
    "dataJoin[dataJoin['pdf文本'] != ''].to_csv('/home/zhuyuhe/mydata/jmd/percent/jmd_text_map.txt', header = None, sep = '\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('/home/zhuyuhe/mydata/jmd/percent/jmd_text_map.txt', header = None, sep = '\\t', encoding = 'utf-8')\n",
    "d.columns = ['index','文本', '意图', 'pdfurl', 'pdf文本']\n",
    "d = d[d['文本'].apply(lambda x: x.__len__() > 10)]\n",
    "d = d[d['文本'].duplicated() == False]\n",
    "d.to_csv('/home/zhuyuhe/mydata/jmd/percent/jmd_text_map.txt', header = None,index=None, sep = '\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = \"创显将大力发展软硬件结合的教学解决方案，配套以不同层次的专业培训服务\"\n",
    "pdflist = ['2017年，公司将大力发展软硬件结合的视频教学解决方案，形成精品录播室，标准录播，配套以不同的专业培训服务', '2017年，公司将在既有的教师化信息能力平台业务基础上，进一步深化细化培养服务']\n",
    "print(isMatch(a, pdflist, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdfurl = ''\n",
    "savePath = '/home/zhuyuhe/mydata/jmd/percent/'\n",
    "fileObject = open(savePath + 'jmd_pdf_map.txt', 'w') \n",
    "def write_unicode(text, charset='utf-8'):\n",
    "    return text.encode(charset)\n",
    "def MatchTwoText(t1, t2, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    检查两个句子是否匹配\n",
    "    \"\"\"\n",
    "#     print t1\n",
    "    ct1 = ' '.join(jieba.cut(t1)).split()\n",
    "    ct2 = ' '.join(jieba.cut(t2)).split()\n",
    "    commonL = set(ct1).intersection(set(ct2)).__len__()\n",
    "#     print commonL\n",
    "#     print ct1\n",
    "#     print set(ct1).__len__()\n",
    "    if commonL * 1.0 / set(ct1).__len__() > threshold:\n",
    "#         print(\"match \" + str(commonL * 1.0/ set(ct1).__len__() * 100) + '% ')\n",
    "#         print(\"文章句子:\" + '\\n' + t1)\n",
    "#         print(\"PDF句子: \" + '\\n' + t2)\n",
    "#         print('\\n')\n",
    "        fileObject.write(t1.encode('utf-8'))\n",
    "        fileObject.write('\\t')\n",
    "        fileObject.write(t2.encode('utf-8'))\n",
    "        fileObject.write('\\t')\n",
    "        fileObject.write(pdfurl)\n",
    "        fileObject.write('\\n')\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def isMatch(text, pdfTextList, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    @parameter text: 待检查句子\n",
    "    @parameter pdfTextList: PDF句子与表格信息列表\n",
    "    @parameter threshold: 相似度为多少时返回true\n",
    "    @return： boolean值，pdf列表中是否有与待检查句子非常相似的句子\n",
    "    \"\"\"\n",
    "    for pdftext in pdfTextList:\n",
    "        if MatchTwoText(text, pdftext, threshold):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_node_texts(pdf_nodes):\n",
    "    '''Given a list of nodes, get the texts of these nodes\n",
    "    '''\n",
    "    texts = []\n",
    "    for node in pdf_nodes:\n",
    "        texts.append(node.obj.get_text())\n",
    "    return texts\n",
    "\n",
    "#percent: 取pdf的前多少页\n",
    "def extractPDF(pdfPath, percent, source = ['table', 'text']):\n",
    "    savePath = '/home/zhuyuhe/mydata/jmd/percent/'\n",
    "    ex = extractor.StockFinanceExtractor(pdfPath, percent)\n",
    "\n",
    "    nodes = ex.get_pdf_objects(filter_headline=True)\n",
    "\n",
    "    textList = []\n",
    "    textNodes = []\n",
    "    tableList = []\n",
    "    tableNodes = []\n",
    "    for node in nodes:\n",
    "        if isinstance(node.obj, pdfextractor.page.Table):\n",
    "            if 'table' in source:\n",
    "                tableNodes.append(node)\n",
    "        elif (not isinstance(node.obj, pdfextractor.pdftree.VirtualNode)) & (not isinstance(node.obj, pdfextractor.page.Catalog)):\n",
    "            if 'text' in source:\n",
    "                textNodes.append(node)\n",
    "\n",
    "    tableList = get_node_texts(tableNodes)\n",
    "    tableList = [t.replace('|', '').split('\\n') for t in tableList]\n",
    "    #二维数组转化为一维\n",
    "    tableList = sum(tableList,[])\n",
    "    textList = get_node_texts(textNodes)\n",
    "    textList = sum([t.encode('utf-8').split('。') for t in textList],[])\n",
    "    textList = [t.replace('\\n', '') for t in textList]\n",
    "    textList = filter(lambda x: x != '', textList)\n",
    "    \n",
    "    pdf = []\n",
    "    pdf.append(textList)\n",
    "    pdf.append(tableList)\n",
    "    pdf = sum(pdf, [])\n",
    "    print pdf.__len__()\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "#     \n",
    "#     fileObject = open(savePath + 'cxkjTextList.txt', 'w')  \n",
    "#     for text in textList:  \n",
    "#         fileObject.write(text)  \n",
    "#         fileObject.write('\\n')\n",
    "#     fileObject.close()\n",
    "#     fileObject2 = open(savePath + 'cxkjTableList.txt', 'w')  \n",
    "#     for table in tableList:  \n",
    "#         fileObject2.write(write_unicode(table))  \n",
    "#         fileObject2.write('\\n')  \n",
    "#     fileObject2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1: \n",
      "http://www.cninfo.com.cn/finalpage/2017-04-19/1203340942.PDF\n",
      "Download pdf time: 0s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 48s\n",
      "Construct Pdf Tree time: 0s\n",
      "1658\n",
      "**************************\n",
      "58.3333333333% 的句子来自PDF\n",
      "***************************\n",
      "Article 2: \n",
      "http://disclosure.szse.cn/finalpage/2017-04-21/1203344261.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 69s\n",
      "Construct Pdf Tree time: 0s\n",
      "2345\n",
      "**************************\n",
      "55.5555555556% 的句子来自PDF\n",
      "***************************\n",
      "Article 3: \n",
      "http://static.sse.com.cn/disclosure/listedinfo/announcement/c/2017-04-15/601801_2016_n.pdf\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 53s\n",
      "Construct Pdf Tree time: 0s\n",
      "3614\n",
      "**************************\n",
      "41.6666666667% 的句子来自PDF\n",
      "***************************\n",
      "Article 4: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-25/1202780794.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 8s\n",
      "Construct Pdf Tree time: 0s\n",
      "494\n",
      "**************************\n",
      "64.2857142857% 的句子来自PDF\n",
      "***************************\n",
      "Article 5: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-18/1202763826.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 8s\n",
      "Construct Pdf Tree time: 0s\n",
      "450\n",
      "**************************\n",
      "37.5% 的句子来自PDF\n",
      "***************************\n",
      "Article 6: \n",
      "http://disclosure.szse.cn/finalpage/2016-04-26/1202239317.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 10s\n",
      "Construct Pdf Tree time: 0s\n",
      "473\n",
      "**************************\n",
      "54.5454545455% 的句子来自PDF\n",
      "***************************\n",
      "Article 7: \n",
      "http://www.cninfo.com.cn/finalpage/2017-04-19/1203341493.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 55s\n",
      "Construct Pdf Tree time: 0s\n",
      "2843\n",
      "**************************\n",
      "45.4545454545% 的句子来自PDF\n",
      "***************************\n",
      "Article 8: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-18/1202765393.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 10s\n",
      "Construct Pdf Tree time: 0s\n",
      "466\n",
      "**************************\n",
      "28.5714285714% 的句子来自PDF\n",
      "***************************\n",
      "Article 9: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-25/1202780960.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 10s\n",
      "Construct Pdf Tree time: 0s\n",
      "486\n",
      "**************************\n",
      "10.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 10: \n",
      "http://www.cninfo.com.cn/finalpage/2017-04-11/1203285646.PDF\n",
      "Download pdf time: 4s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 58s\n",
      "Article 11: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-27/1202791188.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 12s\n",
      "Construct Pdf Tree time: 0s\n",
      "564\n",
      "**************************\n",
      "55.5555555556% 的句子来自PDF\n",
      "***************************\n",
      "Article 12: \n",
      "http://disclosure.szse.cn/finalpage/2017-07-28/1203739610.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 72s\n",
      "Construct Pdf Tree time: 0s\n",
      "1844\n",
      "**************************\n",
      "58.3333333333% 的句子来自PDF\n",
      "***************************\n",
      "Article 13: \n",
      "http://disclosure.szse.cn/finalpage/2016-04-26/1202239571.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 6s\n",
      "Construct Pdf Tree time: 0s\n",
      "363\n",
      "**************************\n",
      "0.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 14: \n",
      "http://disclosure.szse.cn/finalpage/2017-04-26/1203386046.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 9s\n",
      "Construct Pdf Tree time: 0s\n",
      "425\n",
      "**************************\n",
      "66.6666666667% 的句子来自PDF\n",
      "***************************\n",
      "Article 15: \n",
      "http://static.sse.com.cn/disclosure/listedinfo/announcement/c/2017-04-29/600661_2017_1.pdf\n",
      "Download pdf time: 0s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 5s\n",
      "Construct Pdf Tree time: 0s\n",
      "418\n",
      "**************************\n",
      "0.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 16: \n",
      "http://www.cninfo.com.cn/finalpage/2017-06-20/1203635303.PDF\n",
      "Download pdf time: 0s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 46s\n",
      "Construct Pdf Tree time: 0s\n",
      "2216\n",
      "**************************\n",
      "46.1538461538% 的句子来自PDF\n",
      "***************************\n",
      "Article 17: \n",
      "http://disclosure.szse.cn/finalpage/2017-08-10/1203778984.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 68s\n",
      "Construct Pdf Tree time: 0s\n",
      "2333\n",
      "**************************\n",
      "60.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 18: \n",
      "http://disclosure.szse.cn/finalpage/2017-03-28/1203210341.PDF\n",
      "Download pdf time: 8s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 70s\n",
      "Construct Pdf Tree time: 0s\n",
      "2456\n",
      "**************************\n",
      "78.9473684211% 的句子来自PDF\n",
      "***************************\n",
      "Article 19: \n",
      "http://static.sse.com.cn/disclosure/listedinfo/announcement/c/2017-04-29/600730_2017_1.pdf\n",
      "Download pdf time: 0s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 6s\n",
      "Construct Pdf Tree time: 0s\n",
      "421\n",
      "**************************\n",
      "0.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 20: \n",
      "http://static.sse.com.cn/disclosure/listedinfo/announcement/c/2016-08-25/600661_2016_z.pdf\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 32s\n",
      "Construct Pdf Tree time: 0s\n",
      "1950\n",
      "**************************\n",
      "70.5882352941% 的句子来自PDF\n",
      "***************************\n",
      "Article 21: \n",
      "http://www.cninfo.com.cn/finalpage/2017-07-28/1203751786.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 24s\n",
      "Construct Pdf Tree time: 0s\n",
      "1625\n",
      "**************************\n",
      "31.8181818182% 的句子来自PDF\n",
      "***************************\n",
      "Article 22: \n",
      "http://disclosure.szse.cn/finalpage/2017-04-25/1203379575.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 68s\n",
      "Construct Pdf Tree time: 1s\n",
      "2369\n",
      "**************************\n",
      "76.9230769231% 的句子来自PDF\n",
      "***************************\n",
      "Article 23: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-27/1202793355.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 13s\n",
      "Construct Pdf Tree time: 0s\n",
      "491\n",
      "**************************\n",
      "28.5714285714% 的句子来自PDF\n",
      "***************************\n",
      "Article 24: \n",
      "http://www.cninfo.com.cn/finalpage/2017-05-10/1203508904.PDF\n",
      "Download pdf time: 0s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 46s\n",
      "Construct Pdf Tree time: 0s\n",
      "2795\n",
      "**************************\n",
      "77.7777777778% 的句子来自PDF\n",
      "***************************\n",
      "Article 25: \n",
      "http://disclosure.szse.cn/finalpage/2017-04-24/1203360123.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 77s\n",
      "Construct Pdf Tree time: 0s\n",
      "2621\n",
      "**************************\n",
      "47.0588235294% 的句子来自PDF\n",
      "***************************\n",
      "Article 26: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-29/1202801424.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 8s\n",
      "Construct Pdf Tree time: 0s\n",
      "453\n",
      "**************************\n",
      "7.69230769231% 的句子来自PDF\n",
      "***************************\n",
      "Article 27: \n",
      "http://www.cninfo.com.cn/finalpage/2017-04-27/1203451818.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 47s\n",
      "Construct Pdf Tree time: 0s\n",
      "2334\n",
      "**************************\n",
      "58.3333333333% 的句子来自PDF\n",
      "***************************\n",
      "Article 28: \n",
      "http://disclosure.szse.cn/finalpage/2017-03-18/1203174359.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 51s\n",
      "Construct Pdf Tree time: 0s\n",
      "2577\n",
      "**************************\n",
      "40.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 29: \n",
      "http://disclosure.szse.cn/finalpage/2017-08-25/1203872097.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 51s\n",
      "Construct Pdf Tree time: 0s\n",
      "1930\n",
      "**************************\n",
      "25.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 30: \n",
      "http://disclosure.szse.cn/finalpage/2016-04-30/1202283035.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 7s\n",
      "Construct Pdf Tree time: 0s\n",
      "373\n",
      "**************************\n",
      "0.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 31: \n",
      "http://www.cninfo.com.cn/finalpage/2017-04-21/1203369775.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 35s\n",
      "Construct Pdf Tree time: 0s\n",
      "1974\n",
      "**************************\n",
      "61.5384615385% 的句子来自PDF\n",
      "***************************\n",
      "Article 32: \n",
      "http://www.cninfo.com.cn/finalpage/2017-07-26/1203737887.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 24s\n",
      "Construct Pdf Tree time: 0s\n",
      "1529\n",
      "**************************\n",
      "50.0% 的句子来自PDF\n",
      "***************************\n",
      "Article 33: \n",
      "http://disclosure.szse.cn/finalpage/2017-03-21/1203181705.PDF\n",
      "Download pdf time: 3s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 100s\n",
      "Construct Pdf Tree time: 0s\n",
      "2873\n",
      "**************************\n",
      "66.6666666667% 的句子来自PDF\n",
      "***************************\n",
      "Article 34: \n",
      "http://disclosure.szse.cn/finalpage/2016-10-26/1202785562.PDF\n",
      "Download pdf time: 1s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 8s\n",
      "Construct Pdf Tree time: 0s\n",
      "514\n",
      "**************************\n",
      "42.8571428571% 的句子来自PDF\n",
      "***************************\n",
      "Article 35: \n",
      "http://disclosure.szse.cn/finalpage/2017-04-11/1203274908.PDF\n",
      "Download pdf time: 2s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 72s\n",
      "Construct Pdf Tree time: 0s\n",
      "2084\n",
      "**************************\n",
      "69.2307692308% 的句子来自PDF\n",
      "***************************\n",
      "Article 36: \n",
      "http://disclosure.szse.cn/finalpage/2016-08-26/1202622898.PDF\n",
      "Download pdf time: 4s\n",
      "download http status code: 200\n",
      "Pdfminer Parsed time: 46s\n",
      "Construct Pdf Tree time: 0s\n",
      "1732\n",
      "**************************\n",
      "22.2222222222% 的句子来自PDF\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "savePath = '/home/zhuyuhe/mydata/jmd/percent/'\n",
    "fp = open('/home/zhuyuhe/mydata/jmd/percent/jmd_pdf_mapping2.pkl', 'rb')\n",
    "jmdata_mapping = pickle.load(fp)\n",
    "\n",
    "content_url_map = {}\n",
    "for d in jmdata_mapping[2:]:\n",
    "    content_url_map[d['pdf_url']] = d['content']\n",
    "\n",
    "urls = content_url_map.keys()\n",
    "\n",
    "num = 1\n",
    "result = {}\n",
    " \n",
    "\n",
    "for path in urls:\n",
    "    print \"Article \" + str(num) + \": \" \n",
    "    num += 1\n",
    "    \n",
    "    print path\n",
    "    pdfurl = path\n",
    "    \n",
    "    cxkj = content_url_map[path]\n",
    "    #print cxkj\n",
    "    cxkj = cxkj.strip().encode('utf-8').split('。')\n",
    "    cxkj = [x.replace('\\n','') for x in cxkj]\n",
    "    try :\n",
    "        pdflist = extractPDF(path, 0.8)\n",
    "    except BaseException:\n",
    "        continue\n",
    "    \n",
    "#     fileObject = open(savePath + 'cxkjTextList.txt')\n",
    "#     textlist = fileObject.read().split('\\n')\n",
    "#     fileObject = open(savePath + 'cxkjTableList.txt')\n",
    "#     tablelist = fileObject.read().split('\\n')\n",
    "#     fileObject.close()\n",
    "\n",
    "#     PDFTextList = []\n",
    "#     PDFTextList.append(textlist)\n",
    "#     PDFTextList.append(tablelist)\n",
    "#     PDFTextList = sum(PDFTextList, [])\n",
    "#     print PDFTextList.__len__()\n",
    "    print('**************************')\n",
    "    count = 0\n",
    "    for text in cxkj:\n",
    "        if not (len(text) == 0):\n",
    "            if isMatch(text, pdflist, 0.6):\n",
    "                count+=1\n",
    "    print str(count * 1.0/ cxkj.__len__() * 100) + '% 的句子来自PDF'\n",
    "    result[path] = count * 1.0/ cxkj.__len__() * 100\n",
    "    print('***************************')\n",
    "    \n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdftext = dataJoin[dataJoin['pdfurl'] == 'http://disclosure.szse.cn/finalpage/2016-10-27/1202793355.PDF'][dataJoin['pdf文本'] != '']['pdf文本']\n",
    "pdftext.to_csv('/home/zhuyuhe/mydata/jmd/predict/pdftext.txt', header = None, index=None, sep = '\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#计算pdf意图符合文章意图的句子比例\n",
    "pdftext = dataJoin[dataJoin['pdfurl'] == 'http://disclosure.szse.cn/finalpage/2016-10-27/1202793355.PDF'][dataJoin['pdf文本'] != '']\n",
    "pdftext_intent = pd.read_csv('/home/zhuyuhe/mydata/jmd/predict/cxkjpdftext_predict.txt', header = None)\n",
    "pdftext['pdfintent'] = pdftext_intent.values\n",
    "pdftext['pdfintent'] = pdftext['pdfintent'].apply(lambda x: x.replace('__label__', ''))\n",
    "pdftext['意图'] = pdftext['意图'].apply(lambda x: '|'.join(x.split('|')[:2]))\n",
    "bol = pdftext['意图'] == pdftext['pdfintent']\n",
    "print \"文本句子有 \" + str(len(bol)) + \" 句\"\n",
    "print \"pdf意图符合文章意图的句子比例为\" + str(bol.value_counts()[1] * 1.0 / len(bol) * 100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print qtjypdftext[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "qtjypdftext = extractPDF('http://disclosure.szse.cn/finalpage/2016-10-27/1202793355.PDF',0.8, ['text'])\n",
    "qtjypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result = np.array(result.values())\n",
    "result[result != 0]\n",
    "#0%的原因： 1.科大讯飞  -  表格描述；上一季度财报描述  ×2 \n",
    "#           2.新南洋    -  财报中大部分均为表格，并且文章使用了其他季度的财报\n",
    "#           3.中国高科  -  前两句话描述财报表格，\n",
    "#           4.洪涛股份  -  文章与财报不匹配，2015年年报 = 2016年第一季度财报？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#percent: 取pdf的前多少页\n",
    "def extractPDF(pdfPath, percent, source = ['table', 'text']):\n",
    "    savePath = '/home/zhuyuhe/mydata/jmd/percent/'\n",
    "    ex = extractor.StockFinanceExtractor(pdfPath, percent)\n",
    "\n",
    "    nodes = ex.get_pdf_objects(filter_headline=True)\n",
    "\n",
    "    textList = []\n",
    "    textNodes = []\n",
    "    tableList = []\n",
    "    tableNodes = []\n",
    "    for node in nodes:\n",
    "        if isinstance(node.obj, pdfextractor.page.Table):\n",
    "            if 'table' in source:\n",
    "                tableNodes.append(node)\n",
    "        elif (not isinstance(node.obj, pdfextractor.pdftree.VirtualNode)) & (not isinstance(node.obj, pdfextractor.page.Catalog)):\n",
    "            if 'text' in source:\n",
    "                textNodes.append(node)\n",
    "\n",
    "    tableList = get_node_texts(tableNodes)\n",
    "    tableList = [t.replace('|', '').split('\\n') for t in tableList]\n",
    "    #二维数组转化为一维\n",
    "    tableList = sum(tableList,[])\n",
    "    textList = get_node_texts(textNodes)\n",
    "    textList = sum([t.encode('utf-8').split('。') for t in textList],[])\n",
    "    textList = [t.replace('\\n', '') for t in textList]\n",
    "    textList = filter(lambda x: x != '', textList)\n",
    "    \n",
    "    pdf = []\n",
    "    pdf.append(textList)\n",
    "    pdf.append(tableList)\n",
    "    pdf = sum(pdf, [])\n",
    "    print pdf.__len__()\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "#     def write_unicode(text, charset='utf-8'):\n",
    "#         return text.encode(charset)\n",
    "#     fileObject = open(savePath + 'cxkjTextList.txt', 'w')  \n",
    "#     for text in textList:  \n",
    "#         fileObject.write(text)  \n",
    "#         fileObject.write('\\n')\n",
    "#     fileObject.close()\n",
    "#     fileObject2 = open(savePath + 'cxkjTableList.txt', 'w')  \n",
    "#     for table in tableList:  \n",
    "#         fileObject2.write(write_unicode(table))  \n",
    "#         fileObject2.write('\\n')  \n",
    "#     fileObject2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_nodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for n in nodes:\n",
    "    if isinstance(n.obj, pdfextractor.pdftree.VirtualNode):\n",
    "        continue\n",
    "    if len(n.obj.get_text()) <= 25:\n",
    "        continue\n",
    "    else:\n",
    "        text = n.obj.get_text()\n",
    "        if u'□' in text or u'√' in text:\n",
    "            continue\n",
    "        text_nodes.append(n)\n",
    "        print n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "node= text_nodes[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "isinstance(node.obj,pdfextractor.paragraph.Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_child_headlines(pdf_node):\n",
    "    '''Get headlines of children of a node\n",
    "    '''\n",
    "    headlines = []\n",
    "    for node in pdf_node.children:\n",
    "        if isinstance(node.obj, pdfextractor.paragraph.Headline):\n",
    "            headlines.append(node)\n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_child_paragraphs(pdf_node):\n",
    "    '''Get paragraphs of children of a node\n",
    "    '''\n",
    "    paragraphs = []\n",
    "    for node in pdf_node.children:\n",
    "        if isinstance(node.obj, pdfextractor.paragraph.Paragraph):\n",
    "            paragraphs.append(node)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_headlines(pdf_node):\n",
    "    '''Get headlines of a paragraph\n",
    "    Input:\n",
    "        - pdf_node: a pdf node\n",
    "    Output:\n",
    "        - headlines: a list of sorted headline nodes\n",
    "        - texts: a list of sorted headline texts\n",
    "    '''\n",
    "    headlines = []\n",
    "    node = pdf_node.parent\n",
    "    while not isinstance(node.obj,pdfextractor.pdftree.VirtualNode):\n",
    "        headlines.append(node)\n",
    "        node = node.parent\n",
    "    headlines = list(reversed(headlines))\n",
    "    return headlines, get_node_texts(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print get_paragraph_headlines(text_nodes[15])[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "host = 'https://alpha-surreal.aidigger.com/api/v1/classification/intent/finance/0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hd_intents = []\n",
    "all_intents = []\n",
    "for node in text_nodes:\n",
    "    content = node.obj.get_text()\n",
    "    headline = get_paragraph_headlines(node)[1][-1]\n",
    "    intents = requests.post(host,json={\"query\":content}).json()\n",
    "    head_intents = requests.post(host,json={\"query\":headline}).json()\n",
    "    for i,item in enumerate(head_intents['predict']):\n",
    "        intents['predict'][i]['prob'] += item['prob']\n",
    "    all_intents.append(intents)\n",
    "    intent = max(intents['predict'],key=lambda item:item['prob'])\n",
    "    hd_intents.append(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_intent(it):\n",
    "    for i,item in enumerate(it):\n",
    "        print i,item['name'],item['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_intent(all_intents[0]['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,intent in enumerate(hd_intents):\n",
    "    print i\n",
    "    print text_nodes[i]\n",
    "    print ' ==> '.join(get_paragraph_headlines(text_nodes[i])[1])\n",
    "    print intent['name'],'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [zhuEnv2]",
   "language": "python",
   "name": "Python [zhuEnv2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
